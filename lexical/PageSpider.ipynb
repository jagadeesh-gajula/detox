{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing all the libaries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import PrettyPrinter\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing all the datasets required for application\n",
    "data=requests.get('http://facebook.com')\n",
    "data=data.text\n",
    "data=BeautifulSoup(data,'lxml')\n",
    "\n",
    "df=pd.read_csv('hist_data.csv',encoding = \"cp1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ths clean function will remove all the noncense from the data\n",
    "# how ever we need to process NLTK before fed into model\n",
    "def clean(string):\n",
    "    specials=['!','@','#','$','%','^','&','*','(',')','_','-','+','`','~','?','|','/','\"','=','<','>','{','}','[',']'\\\n",
    "             ';',':','.',',','©','®','1','2','3','4','5','6','7','8','9','0']\n",
    "    for special in specials:    \n",
    "        string=string.replace(special,\" \")\n",
    "    return string\n",
    "\n",
    "def getit(url):\n",
    "        seq=[]\n",
    "        data=requests.get(str(url))\n",
    "        if data.status_code != 200:\n",
    "            print(\"request failed code: \",data.request)\n",
    "        data=data.text\n",
    "        data=BeautifulSoup(data,'lxml')\n",
    "        p=data.find_all('p')\n",
    "        p.append(data.find_all('a'))\n",
    "        p.append(data.find_all('h1'))\n",
    "        p.append(data.find_all('h2'))\n",
    "        p.append(data.find_all('h5'))\n",
    "        p.append(data.find_all('td'))\n",
    "        p.append(data.find_all('title'))\n",
    "        p.append(data.find_all('b'))\n",
    "        p.append(data.find_all('i'))\n",
    "        p.append(data.find_all('label'))\n",
    "        string=p\n",
    "        string=str(string)\n",
    "        string=string.split(',')\n",
    "        for i in string:\n",
    "            begin=i.find('>')\n",
    "            i=i[begin+1:]\n",
    "            end=i.find('<')\n",
    "            i=i[:end]\n",
    "            if len(i)==1 or len(i)==2:\n",
    "                break\n",
    "            seq.append(clean(i.strip()))\n",
    "        s=\" \"\n",
    "        return s.join(seq)\n",
    "        \n",
    "        \n",
    "#for link in range(df.shape[0]):\n",
    "#    url=hi=df.iloc[[link],[0]].values[0][0]\n",
    "#    print(url,\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "#    print(getit(url))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getit('http://www.hdpornpics.com')\n",
    "getit('http://thumbzilla.com')\n",
    "getit('http://mobilekeez.com')\n",
    "getit('http://spankbang.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('adult.csv')\n",
    "\n",
    "\n",
    "for i in range(20,30):\n",
    "    u=df.iloc[[i],[0]].values[0][0]\n",
    "    u=str(u)\n",
    "    print(u,\"===\",getit(u))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
